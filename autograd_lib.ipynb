{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U8oxpVm4PXTT"
   },
   "outputs": [],
   "source": [
    "%run \"caching.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "V91YDX__GVJK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lint as python3\n",
    "# Copyright 2019 Google LLC.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# pylint: disable=missing-docstring\n",
    "# pylint: disable=unused-argument\n",
    "# pylint: disable=g-import-not-at-top\n",
    "\n",
    "import warnings\n",
    "\n",
    "import autograd\n",
    "import autograd.core\n",
    "import autograd.extend\n",
    "import autograd.numpy as anp\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "# try:\n",
    "#   import sksparse.cholmod\n",
    "#   HAS_CHOLMOD = True\n",
    "# except ImportError:\n",
    "#   warnings.warn(\n",
    "#       'sksparse.cholmod not installed. Falling back to SciPy/SuperLU, but '\n",
    "#       'simulations will be about twice as slow.')\n",
    "#   HAS_CHOLMOD = False\n",
    "\n",
    "\n",
    "# internal utilities\n",
    "def _grad_undefined(_, *args):\n",
    "  raise TypeError('gradient undefined for this input argument')\n",
    "\n",
    "\n",
    "def _zero_grad(_, *args, **kwargs):\n",
    "  def jvp(grad_ans):\n",
    "    return 0.0 * grad_ans\n",
    "  return jvp\n",
    "\n",
    "\n",
    "# Gaussian filter\n",
    "@autograd.extend.primitive\n",
    "def gaussian_filter(x, width):\n",
    "  \"\"\"Apply gaussian blur of a given radius.\"\"\"\n",
    "  return scipy.ndimage.gaussian_filter(x, width, mode='reflect')\n",
    "\n",
    "\n",
    "def _gaussian_filter_vjp(ans, x, width):\n",
    "  del ans, x  # unused\n",
    "  return lambda g: gaussian_filter(g, width)\n",
    "autograd.extend.defvjp(gaussian_filter, _gaussian_filter_vjp)\n",
    "\n",
    "\n",
    "# Cone filter\n",
    "def _cone_filter_matrix(nelx, nely, radius, mask):\n",
    "  x, y = np.meshgrid(np.arange(nelx), np.arange(nely), indexing='ij')\n",
    "\n",
    "  rows = []\n",
    "  cols = []\n",
    "  values = []\n",
    "  r_bound = int(np.ceil(radius))\n",
    "  for dx in range(-r_bound, r_bound+1):\n",
    "    for dy in range(-r_bound, r_bound+1):\n",
    "      weight = np.maximum(0, radius - np.sqrt(dx**2 + dy**2))\n",
    "      row = x + nelx * y\n",
    "      column = x + dx + nelx * (y + dy)\n",
    "      value = np.broadcast_to(weight, x.shape)\n",
    "\n",
    "      # exclude cells beyond the boundary\n",
    "      valid = (\n",
    "          (mask > 0) &\n",
    "          ((x+dx) >= 0) &\n",
    "          ((x+dx) < nelx) &\n",
    "          ((y+dy) >= 0) &\n",
    "          ((y+dy) < nely)\n",
    "      )\n",
    "      rows.append(row[valid])\n",
    "      cols.append(column[valid])\n",
    "      values.append(value[valid])\n",
    "\n",
    "  data = np.concatenate(values)\n",
    "  i = np.concatenate(rows)\n",
    "  j = np.concatenate(cols)\n",
    "  return scipy.sparse.coo_matrix((data, (i, j)), (nelx * nely,) * 2)\n",
    "\n",
    "\n",
    "@ndarray_safe_lru_cache()\n",
    "def normalized_cone_filter_matrix(nx, ny, radius, mask):\n",
    "  \"\"\"Calculate a sparse matrix appropriate for applying a cone filter.\"\"\"\n",
    "  raw_filters = _cone_filter_matrix(nx, ny, radius, mask).tocsr()\n",
    "  weights = 1 / raw_filters.sum(axis=0).squeeze()\n",
    "  diag_weights = scipy.sparse.spdiags(weights, 0, nx*ny, nx*ny)\n",
    "  return (diag_weights @ raw_filters).tocsr()\n",
    "\n",
    "\n",
    "@autograd.extend.primitive\n",
    "def cone_filter(inputs, radius, mask=1, transpose=False):\n",
    "  \"\"\"Apply a cone filter of the given radius.\"\"\"\n",
    "  inputs = np.asarray(inputs)\n",
    "  filters = normalized_cone_filter_matrix(\n",
    "      *inputs.shape, radius=radius, mask=mask)\n",
    "  if transpose:\n",
    "    filters = filters.T\n",
    "  outputs = filters @ inputs.ravel(order='F')\n",
    "  return outputs.reshape(inputs.shape, order='F')\n",
    "\n",
    "\n",
    "def _cone_filter_vjp(ans, inputs, radius, mask=1, transpose=False):\n",
    "  del ans, inputs  # unused\n",
    "  return lambda g: cone_filter(g, radius, mask, transpose=not transpose)\n",
    "autograd.extend.defvjp(cone_filter, _cone_filter_vjp)\n",
    "\n",
    "\n",
    "## a useful utility for 1D scatter operations\n",
    "def inverse_permutation(indices):\n",
    "  inverse_perm = np.zeros(len(indices), dtype=anp.int64)\n",
    "  inverse_perm[indices] = np.arange(len(indices), dtype=anp.int64)\n",
    "  return inverse_perm\n",
    "\n",
    "\n",
    "# the 1D scatter operation\n",
    "def scatter1d(nonzero_values, nonzero_indices, array_len):\n",
    "  all_indices = np.arange(array_len, dtype=anp.int64)\n",
    "  zero_indices = anp.setdiff1d(all_indices, nonzero_indices, assume_unique=True)\n",
    "  index_map = inverse_permutation(\n",
    "      anp.concatenate([nonzero_indices, zero_indices]))\n",
    "  u_values = anp.concatenate([nonzero_values, anp.zeros(len(zero_indices))])\n",
    "  return u_values[index_map]\n",
    "\n",
    "\n",
    "@ndarray_safe_lru_cache(1)\n",
    "def _get_solver(a_entries, a_indices, size, sym_pos):\n",
    "  \"\"\"Get a solver for applying the desired matrix factorization.\"\"\"\n",
    "  # A cache size of one is sufficient to avoid re-computing the factorization in\n",
    "  # the backwawrds pass.\n",
    "  a = scipy.sparse.coo_matrix((a_entries, a_indices), shape=(size,)*2).tocsc()\n",
    "  return scipy.sparse.linalg.splu(a).solve\n",
    "\n",
    "\n",
    "## Sparse solver\n",
    "@autograd.primitive\n",
    "def solve_coo(a_entries, a_indices, b, sym_pos=False):\n",
    "  \"\"\"Solve a sparse system of linear equations.\n",
    "  Args:\n",
    "    a_entries: numpy array with shape (num_zeros,) giving values for non-zero\n",
    "      matrix entries.\n",
    "    a_indices: numpy array with shape (2, num_zeros) giving x and y indices for\n",
    "      non-zero matrix entries.\n",
    "    b: 1d numpy array specifying the right hand side of the equation.\n",
    "    sym_pos: is the matrix guaranteed to be positive-definite?\n",
    "  Returns:\n",
    "    1d numpy array corresponding to the solution of a*x=b.\n",
    "  \"\"\"\n",
    "  solver = _get_solver(a_entries, a_indices, b.size, sym_pos)\n",
    "  return solver(b)\n",
    "\n",
    "\n",
    "# see autograd's np.linalg.solve:\n",
    "# https://github.com/HIPS/autograd/blob/96a03f44da43cd7044c61ac945c483955deba957/autograd/numpy/linalg.py#L40\n",
    "\n",
    "\n",
    "def solve_coo_adjoint(a_entries, a_indices, b, sym_pos=False):\n",
    "  # NOTE: not tested on complex valued inputs.\n",
    "  if sym_pos:\n",
    "    return solve_coo(a_entries, a_indices, b, sym_pos)\n",
    "  else:\n",
    "    return solve_coo(a_entries, a_indices[::-1], b, sym_pos)\n",
    "\n",
    "\n",
    "def grad_solve_coo_entries(ans, a_entries, a_indices, b, sym_pos=False):\n",
    "  def jvp(grad_ans):\n",
    "    lambda_ = solve_coo_adjoint(a_entries, a_indices, grad_ans, sym_pos)\n",
    "    i, j = a_indices\n",
    "    return -lambda_[i] * ans[j]\n",
    "  return jvp\n",
    "\n",
    "\n",
    "def grad_solve_coo_b(ans, a_entries, a_indices, b, sym_pos=False):\n",
    "  def jvp(grad_ans):\n",
    "    return solve_coo_adjoint(a_entries, a_indices, grad_ans, sym_pos)\n",
    "  return jvp\n",
    "\n",
    "\n",
    "autograd.extend.defvjp(\n",
    "    solve_coo, grad_solve_coo_entries, _grad_undefined, grad_solve_coo_b)\n",
    "\n",
    "\n",
    "@autograd.primitive\n",
    "def find_root(\n",
    "    f, x, lower_bound, upper_bound, tolerance=1e-12, max_iterations=64):\n",
    "  # Implicitly solve f(x,y)=0 for y(x) using binary search.\n",
    "  # Assumes that y is a scalar and f(x,y) is monotonic in y.\n",
    "  for _ in range(max_iterations):\n",
    "    y = 0.5 * (lower_bound + upper_bound)\n",
    "    if upper_bound - lower_bound < tolerance:\n",
    "      break\n",
    "    if f(x, y) > 0:\n",
    "      upper_bound = y\n",
    "    else:\n",
    "      lower_bound = y\n",
    "  return y\n",
    "\n",
    "\n",
    "def grad_find_root(y, f, x, lower_bound, upper_bound, tolerance=None):\n",
    "  # This uses a special case of the adjoint gradient rule:\n",
    "  # http://www.dolfin-adjoint.org/en/latest/documentation/maths/3-gradients.html#the-adjoint-approach\n",
    "  def jvp(grad_y):\n",
    "    g = lambda x: f(x, y)\n",
    "    h = lambda y: f(x, y)\n",
    "    return -autograd.grad(g)(x) / autograd.grad(h)(y) * grad_y\n",
    "  return jvp\n",
    "\n",
    "\n",
    "autograd.extend.defvjp(\n",
    "    find_root, _grad_undefined, grad_find_root,\n",
    "    _zero_grad, _zero_grad, _zero_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.15 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "3cd6af004486e18d8cd1b1dc71eb6e14b35da0a003c4531af785de1b844902cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
