{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kzDKwgoc0Gvn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "from os import makedirs\n",
    "%run \"topo_physics.ipynb\"\n",
    "%run \"utils.ipynb\"\n",
    "layers = tf.keras.layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备训练数据的相关函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Fx7xGe8pxgLW"
   },
   "outputs": [],
   "source": [
    "def block_to_full(blocks, block_size, nely, nelx):\n",
    "  '''将分块重组'''\n",
    "  '''(batch_size, num_of_blocks, block_size**2) into (batch_size, nely, nelx)'''\n",
    "  batch_size, num_of_blocks = blocks.shape[0], blocks.shape[1]\n",
    "  num_of_slice_x, num_of_slice_y = nelx//block_size, nely//block_size\n",
    "  blocks = tf.reshape(blocks, (batch_size, num_of_slice_y, num_of_slice_x, block_size, block_size))\n",
    "  return tf.reshape(tf.experimental.numpy.moveaxis(blocks,2,3),(batch_size, nely, nelx))\n",
    "\n",
    "from skimage.transform import resize\n",
    "def sens_normalization(sens_full, sens_red_block, block_size):\n",
    "  '''使用维度缩减后的单元灵敏度（粗粒）对原尺度（细粒）灵敏度进行归一化'''\n",
    "  num_of_samples = sens_full.shape[0]\n",
    "  nely, nelx = sens_full.shape[1], sens_full.shape[2]\n",
    "  num_of_xslice, num_of_yslice = nelx//block_size, nely//block_size\n",
    "  sens_red_block = sens_red_block.reshape((num_of_samples,num_of_yslice,num_of_xslice)) # (2000,20,20)\n",
    "  sens_red_expanded = resize(sens_red_block, (num_of_samples,nely,nelx), order=0, preserve_range=True)\n",
    "  return sens_full/sens_red_expanded\n",
    "\n",
    "def extract_data(ds_full, ds_block):\n",
    "  '''从data_random.ipynb生成的数据中提取出神经网络输入数据'''\n",
    "  nely, nelx = ds_full.nely.values.shape[0], ds_full.nelx.values.shape[0]\n",
    "  block_size = ds_block.yblock.values.shape[0]\n",
    "  num_of_blocks = nely//block_size * nelx//block_size\n",
    "  num_of_data = ds_full.number.values.shape[0]\n",
    "  dens_full = ds_full.dens_full.values # (2000,60,60)\n",
    "  sens_full = ds_full.sens_full.values\n",
    "  sens_red_block = ds_block.sens_red_block.values\n",
    "  dens_red_block = ds_block.dens_red_block.values\n",
    "  outputs = sens_normalization(sens_full, sens_red_block, block_size)\n",
    "  ux_red_block = ds_block.ux_red_block.values #(1800000,2,2)\n",
    "  uy_red_block = ds_block.uy_red_block.values\n",
    "  disp_slice = np.stack((ux_red_block, uy_red_block), axis=-1).reshape((num_of_data,num_of_blocks,8))\n",
    "  return dens_full, disp_slice, outputs\n",
    "\n",
    "\n",
    "def get_dataset(ds, ds_size, batch_size, train_split=0.8, val_split=0, test_split=0.2, shuffle=True, shuffle_size=10000):\n",
    "    '''数据集洗牌、分割'''\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    \n",
    "    if shuffle:\n",
    "        # Specify seed to always have the same split distribution between runs\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size).batch(batch_size) \n",
    "    val_ds = ds.skip(train_size).take(val_size).batch(batch_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size).batch(batch_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络架构部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DMN8VPKRVPpG"
   },
   "outputs": [],
   "source": [
    "def ConvBlock(inputs, n_filters, kernel_size, strides):\n",
    "    conv = layers.Conv2D(n_filters, kernel_size, strides, activation=None, \n",
    "                         padding='same', kernel_initializer='random_normal')(inputs)\n",
    "    bn = layers.BatchNormalization()(conv, training=True)\n",
    "    activation = layers.ReLU()(bn)\n",
    "    # activation = layers.LeakyReLU()(bn)\n",
    "    # activation = PReLU(bn)\n",
    "    return activation\n",
    "\n",
    "def PReLU(_x):\n",
    "  alphas = tf.compat.v1.get_variable(name=None, shape=_x.get_shape()[-1],\n",
    "                       initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float32)\n",
    "  pos = tf.nn.relu(_x)\n",
    "  neg = alphas * (_x - abs(_x)) * 0.5\n",
    " \n",
    "  return pos + neg\n",
    "\n",
    "def cnn_model(num_of_layers, input_size=(None,None,1), ):\n",
    "  '''input (batch_size, nely, nelx), output (batch_size, nely, nelx, n_filters)'''\n",
    "  inputs = layers.Input((input_size))\n",
    "  conv = ConvBlock(inputs, 64, 3, 1)\n",
    "  for i in range(num_of_layers-1):\n",
    "    conv = ConvBlock(conv, 64, 3, 1)\n",
    "  outputs = layers.Conv2D(filters=4, kernel_size=3, strides=1, activation=None, \n",
    "                         padding='same',kernel_initializer='random_normal')(conv)\n",
    "  outputs = layers.ReLU()(outputs)\n",
    "  # outputs = layers.LeakyReLU()(outputs)\n",
    "  # outputs = PReLU(outputs)\n",
    "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "  return model\n",
    "\n",
    "def features_to_block(features, block_size=2):\n",
    "  '''\n",
    "  将卷积得到的特征图切块\n",
    "  (batch_size, nely, nelx, fillter_size)  into \n",
    "  (batch_size*(nely//block_size)*(nelx//blocksize), block_size, block_size)) blocks\n",
    "  '''\n",
    "  nely, nelx, n_filters = features.shape[1], features.shape[2], features.shape[3]\n",
    "  num_of_slice = nelx//block_size\n",
    "  transformed = tf.experimental.numpy.moveaxis(tf.reshape(features,(-1,block_size,num_of_slice,block_size,n_filters)),1,2)\n",
    "  return tf.reshape(transformed,(-1,block_size,block_size,n_filters))\n",
    "\n",
    "\n",
    "def fnn_model(block_size, num_of_layers):\n",
    "  feature_input = layers.Input((block_size*block_size*4)) # (batch_size, block_size, block_size, n_fil)\n",
    "  disp_input = layers.Input((8))  # (batch_size, 8)\n",
    "  combined = layers.concatenate([feature_input, disp_input])\n",
    "  z = layers.Dense(100,activation=None)(combined)\n",
    "  z = layers.ReLU()(z)\n",
    "  # z = layers.LeakyReLU()(z)\n",
    "  # z = PReLU(z)\n",
    "  for i in range(num_of_layers-1):\n",
    "    z = layers.Dense(100,activation=None)(z)\n",
    "    z = layers.ReLU()(z)\n",
    "    # z = layers.LeakyReLU()(z)\n",
    "    # z = PReLU(z)\n",
    "  z = layers.Dense(block_size**2,activation=None)(z)\n",
    "  # z = layers.LeakyReLU()(z)\n",
    "  z = layers.ReLU()(z)\n",
    "  model = tf.keras.Model(inputs=[feature_input,disp_input], outputs=z)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjt_YroIzQ-_"
   },
   "outputs": [],
   "source": [
    "def custom_train(block_size, cnn_model, fnn_model, loss_function, num_epoch, optimizer, batched_train, batched_test,\n",
    "                 num_of_layers_1, num_of_layers_2, save_model, casename):\n",
    "  '''神经网络训练使用的主函数'''\n",
    "  if save_model:\n",
    "    # 创建文件夹路径\n",
    "    folder_paths = [f'Model/{casename}/Models', f'Model/{casename}/Weights', f'Model/{casename}/Loss history']\n",
    "    for folder_path in folder_paths:          \n",
    "      makedirs(folder_path, exist_ok=True)\n",
    "  best_performance=1e7\n",
    "  cnn_tvars = cnn_model.trainable_variables\n",
    "  fnn_tvars = fnn_model.trainable_variables\n",
    "  test_losses, val_losses = [], []\n",
    "  nely, nelx = 60, 60\n",
    "  num_of_blocks = (nely//block_size) * (nelx//block_size)\n",
    "  for epoch in range(num_epoch):\n",
    "    step_test_losses, step_val_losses = [], []\n",
    "    #测试数据\n",
    "    for step, (input_test, output_test) in enumerate(batched_test):\n",
    "      batch_size = output_test.shape[0]\n",
    "      dens_input, disp_input = input_test['dens_input'], input_test['disp_input']\n",
    "      feature_maps = cnn_model(dens_input)\n",
    "      feature_blocks = features_to_block(feature_maps, block_size)\n",
    "      feature_blocks_pipeline = tf.reshape(feature_blocks,(feature_blocks.shape[0],-1))\n",
    "      disp_blocks_pipeline = tf.reshape(disp_input,(-1,8))\n",
    "      fnn_outputs = fnn_model([feature_blocks_pipeline, disp_blocks_pipeline])\n",
    "      fnn_outputs = tf.reshape(fnn_outputs, (batch_size, num_of_blocks,block_size**2))\n",
    "      output_pred = block_to_full(fnn_outputs, block_size, nely, nelx)\n",
    "      val_loss = loss_function(output_test,output_pred)\n",
    "      step_val_losses.append(val_loss.numpy().item())\n",
    "    #训练数据\n",
    "    for step, (input_train, output_train) in enumerate(batched_train):\n",
    "      with tf.GradientTape() as cnn_tape, tf.GradientTape() as fnn_tape:\n",
    "        batch_size = output_train.shape[0]\n",
    "        dens_input, disp_input = input_train['dens_input'], input_train['disp_input']\n",
    "        cnn_tape.watch(cnn_tvars)\n",
    "        fnn_tape.watch(fnn_tvars)\n",
    "        feature_maps = cnn_model(dens_input)\n",
    "        feature_blocks = features_to_block(feature_maps, block_size)\n",
    "        feature_blocks_pipeline = tf.reshape(feature_blocks,(feature_blocks.shape[0],-1))\n",
    "        disp_blocks_pipeline = tf.reshape(disp_input,(-1,8))\n",
    "        fnn_outputs = fnn_model([feature_blocks_pipeline, disp_blocks_pipeline])\n",
    "        fnn_outputs = tf.reshape(fnn_outputs, (batch_size, num_of_blocks,block_size**2))\n",
    "        output_pred = block_to_full(fnn_outputs, block_size, nely, nelx)\n",
    "        loss = loss_function(output_train,output_pred)\n",
    "        cnn_grads = cnn_tape.gradient(loss, cnn_tvars)\n",
    "        fnn_grads = fnn_tape.gradient(loss, fnn_tvars)\n",
    "        optimizer.apply_gradients(zip(cnn_grads, cnn_tvars))\n",
    "        optimizer.apply_gradients(zip(fnn_grads, fnn_tvars))\n",
    "        step_test_losses.append(loss.numpy().item())\n",
    "\n",
    "    epoch_test_loss, epoch_val_loss = tf.reduce_mean(step_test_losses), tf.reduce_mean(step_val_losses)\n",
    "    if epoch_val_loss < best_performance and save_model:\n",
    "      cnn_model.save_weights(\"Model/\"+casename+\"/Weights/cnn_{layers_1}+{layers_2}layers_{Nb}xblock.h5\".format(layers_1=num_of_layers_1,layers_2=num_of_layers_2, Nb=block_size))\n",
    "      fnn_model.save_weights(\"Model/\"+casename+\"/Weights/fnn_{layers_1}+{layers_2}layers_{Nb}xblock.h5\".format(layers_1=num_of_layers_1,layers_2=num_of_layers_2, Nb=block_size))\n",
    "      best_performance = epoch_val_loss\n",
    "    # if epoch > 1 and epoch % 25 == 0:  \n",
    "    #   cnn_model.load_weights(casename+\"/Weights/cnn_{layers_1}+{layers_2}layers_{Nb}xblock.h5\".format(layers_1=num_of_layers_1,layers_2=num_of_layers_2, Nb=block_size))\n",
    "    #   fnn_model.load_weights(casename+\"/Weights/fnn_{layers_1}+{layers_2}layers_{Nb}xblock.h5\".format(layers_1=num_of_layers_1,layers_2=num_of_layers_2, Nb=block_size))\n",
    "    #   cnn_model.save(casename+\"/Models/cnn_{layers_1}+{layers_2}layers_{Nb}xblock_{epoch}epoch.h5\".format(layers_1=num_of_layers_1,layers_2=num_of_layers_2, Nb=block_size, epoch=epoch))\n",
    "    #   fnn_model.save(casename+\"/Models/fnn_{layers_1}+{layers_2}layers_{Nb}xblock_{epoch}epoch.h5\".format(layers_1=num_of_layers_1,layers_2=num_of_layers_2, Nb=block_size, epoch=epoch)) \n",
    "    test_losses.append(epoch_test_loss)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    print('epoch:',epoch,'loss:',float(epoch_test_loss),'val_loss:',float(epoch_val_loss))\n",
    "  if save_model:\n",
    "    cnn_model.load_weights(\"Model/\"+casename+\"/Weights/cnn_{layers_1}+{layers_2}layers_{Nb}xblock.h5\".format(layers_1=num_of_layers_1,layers_2=num_of_layers_2, Nb=block_size))\n",
    "    fnn_model.load_weights(\"Model/\"+casename+\"/Weights/fnn_{layers_1}+{layers_2}layers_{Nb}xblock.h5\".format(layers_1=num_of_layers_1,layers_2=num_of_layers_2, Nb=block_size))\n",
    "    cnn_model.save(\"Model/\"+casename+\"/Models/cnn_{layers_1}+{layers_2}layers_{Nb}xblock.h5\".format(layers_1=num_of_layers_1,layers_2=num_of_layers_2, Nb=block_size))\n",
    "    fnn_model.save(\"Model/\"+casename+\"/Models/fnn_{layers_1}+{layers_2}layers_{Nb}xblock.h5\".format(layers_1=num_of_layers_1,layers_2=num_of_layers_2, Nb=block_size)) \n",
    "    np.save(\"Model/\"+casename+\"/Loss history/test_{layers_1}+{layers_2}layers_{Nb}xblock.npy\".format(layers_1=num_of_layers_1,layers_2=num_of_layers_2, Nb=block_size),test_losses)\n",
    "    np.save(\"Model/\"+casename+\"/Loss history/val_{layers_1}+{layers_2}layers_{Nb}xblock.npy\".format(layers_1=num_of_layers_1,layers_2=num_of_layers_2, Nb=block_size),val_losses)\n",
    "  print(\"best performance:\", best_performance)\n",
    "  return test_losses, val_losses"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.15 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "3cd6af004486e18d8cd1b1dc71eb6e14b35da0a003c4531af785de1b844902cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
